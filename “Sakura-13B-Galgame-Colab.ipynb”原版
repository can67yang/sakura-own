{"cells":[{"cell_type":"markdown","metadata":{"id":"Tu2Qu_P9sK-G"},"source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Isotr0py/SakuraLLM-Notebooks/blob/main/Sakura-13B-Galgame-Colab.ipynb)"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tvkI52m5DRsL","outputId":"08ccc318-b27d-483b-d6cc-3d3c6d23b580","executionInfo":{"status":"ok","timestamp":1728228029292,"user_tz":-480,"elapsed":25286,"user":{"displayName":"cymhh nory","userId":"17474100186628747229"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n","Sun Oct  6 15:20:28 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n","| N/A   52C    P8              12W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                                         |                      |                  N/A |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","|  No running processes found                                                           |\n","+---------------------------------------------------------------------------------------+\n"]}],"source":["#@title 初始化环境\n","#@markdown 挂载Google网盘\n","Mount_GDrive = True # @param {type:\"boolean\"}\n","if Mount_GDrive:\n","  from google.colab import drive\n","\n","  drive.mount('/content/gdrive')\n","  ROOT_PATH = \"/content/gdrive/MyDrive\"\n","else:\n","  ROOT_PATH = \"/content\"\n","!nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gelzXVWEGxZw"},"outputs":[],"source":["#@title 安装依赖\n","%cd $ROOT_PATH\n","!git clone https://github.com/SakuraLLM/Sakura-13B-Galgame.git\n","\n","%cd Sakura-13B-Galgame\n","!git pull\n","\n","LLAMA_CPP = True # @param {type:\"boolean\"}\n","VLLM = True # @param {type:\"boolean\"}\n","if LLAMA_CPP:\n","  !pip install \"diskcache>=5.6.1\"\n","  !pip install llama-cpp-python -i https://abetlen.github.io/llama-cpp-python/whl/cu122\n","if VLLM:\n","  !pip install -U transformers tokenizers\n","  !pip install vllm\n","!pip install -q -r requirements.txt\n","!pip install -q pyngrok"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"DUQnJQ96Jau8"},"outputs":[],"source":["#@title 翻译EPUB\n","from huggingface_hub import hf_hub_download\n","\n","repo_id = \"SakuraLLM/Sakura-14B-Qwen2beta-v0.9.2-GGUF\"\n","MODEL = \"sakura-14b-qwen2beta-v0.9.2-q4km.gguf\" # @param [\"sakura-14b-qwen2beta-v0.9.2-iq4xs.gguf\", \"sakura-14b-qwen2beta-v0.9.2-q2k.gguf\", \"sakura-14b-qwen2beta-v0.9.2-q3km.gguf\", \"sakura-14b-qwen2beta-v0.9.2-q4km.gguf\", \"sakura-14b-qwen2beta-v0.9.2-q6k.gguf\"]\n","hf_hub_download(repo_id=repo_id, filename=MODEL, local_dir=\"models/\")\n","MODEL_PATH = f\"./models/{MODEL}\"\n","EPUB_PATH = \"novel.epub\" # @param {type:\"string\"}\n","GPT_DICT_PATH = \"dict.txt\"  # @param {type:\"string\"}\n","OUTPUT_FOLDER = \"output/\" # @param {type:\"string\"}\n","\n","%cd $ROOT_PATH/Sakura-13B-Galgame\n","!python translate_epub.py \\\n","    --model_name_or_path $MODEL_PATH \\\n","    --llama_cpp \\\n","    --use_gpu \\\n","    --model_version 0.9 \\\n","    --trust_remote_code \\\n","    --data_path $EPUB_PATH \\\n","    --gpt_dict_path $GPT_DICT_PATH \\\n","    --output_folder $OUTPUT_FOLDER"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S9gYbA1yMVND"},"outputs":[],"source":["#@title 翻译文本文件\n","from huggingface_hub import hf_hub_download\n","\n","repo_id = \"SakuraLLM/Sakura-14B-Qwen2beta-v0.9.2-GGUF\"\n","MODEL = \"sakura-14b-qwen2beta-v0.9.2-q4km.gguf\" # @param [\"sakura-14b-qwen2beta-v0.9.2-iq4xs.gguf\", \"sakura-14b-qwen2beta-v0.9.2-q2k.gguf\", \"sakura-14b-qwen2beta-v0.9.2-q3km.gguf\", \"sakura-14b-qwen2beta-v0.9.2-q4km.gguf\", \"sakura-14b-qwen2beta-v0.9.2-q6k.gguf\"]\n","hf_hub_download(repo_id=repo_id, filename=MODEL, local_dir=\"models/\")\n","MODEL_PATH = f\"./models/{MODEL}\"\n","DATA_PATH = \"novel.txt\" # @param {type:\"string\"}\n","OUTPUT_PATH = \"novel_translated.txt\" # @param {type:\"string\"}\n","\n","%cd $ROOT_PATH/Sakura-13B-Galgame\n","!python translate_novel.py \\\n","    --model_name_or_path $MODEL_PATH \\\n","    --llama_cpp \\\n","    --use_gpu \\\n","    --model_version 0.9 \\\n","    --trust_remote_code \\\n","    --data_path $DATA_PATH \\\n","    --output_path $OUTPUT_PATH"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"https://github.com/Isotr0py/SakuraLLM-Notebooks/blob/main/Sakura-13B-Galgame-Colab.ipynb","timestamp":1728222703685}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}